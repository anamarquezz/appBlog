{"ast":null,"code":"'use strict';\n\nvar _asyncToGenerator = require(\"C:/sources/GitHub/appBlog/BlogLab/BlogLab-UI/node_modules/@babel/runtime/helpers/asyncToGenerator.js\").default;\n\nconst util = require('util');\n\nconst crypto = require('crypto');\n\nconst fs = require('@npmcli/fs');\n\nconst Minipass = require('minipass');\n\nconst path = require('path');\n\nconst ssri = require('ssri');\n\nconst uniqueFilename = require('unique-filename');\n\nconst contentPath = require('./content/path');\n\nconst fixOwner = require('./util/fix-owner');\n\nconst hashToSegments = require('./util/hash-to-segments');\n\nconst indexV = require('../package.json')['cache-version'].index;\n\nconst moveFile = require('@npmcli/move-file');\n\nconst _rimraf = require('rimraf');\n\nconst rimraf = util.promisify(_rimraf);\nrimraf.sync = _rimraf.sync;\nmodule.exports.NotFoundError = class NotFoundError extends Error {\n  constructor(cache, key) {\n    super(`No cache entry for ${key} found in ${cache}`);\n    this.code = 'ENOENT';\n    this.cache = cache;\n    this.key = key;\n  }\n\n};\nmodule.exports.compact = compact;\n\nfunction compact(_x, _x2, _x3) {\n  return _compact.apply(this, arguments);\n}\n\nfunction _compact() {\n  _compact = _asyncToGenerator(function* (cache, key, matchFn, opts = {}) {\n    const bucket = bucketPath(cache, key);\n    const entries = yield bucketEntries(bucket);\n    const newEntries = []; // we loop backwards because the bottom-most result is the newest\n    // since we add new entries with appendFile\n\n    for (let i = entries.length - 1; i >= 0; --i) {\n      const entry = entries[i]; // a null integrity could mean either a delete was appended\n      // or the user has simply stored an index that does not map\n      // to any content. we determine if the user wants to keep the\n      // null integrity based on the validateEntry function passed in options.\n      // if the integrity is null and no validateEntry is provided, we break\n      // as we consider the null integrity to be a deletion of everything\n      // that came before it.\n\n      if (entry.integrity === null && !opts.validateEntry) {\n        break;\n      } // if this entry is valid, and it is either the first entry or\n      // the newEntries array doesn't already include an entry that\n      // matches this one based on the provided matchFn, then we add\n      // it to the beginning of our list\n\n\n      if ((!opts.validateEntry || opts.validateEntry(entry) === true) && (newEntries.length === 0 || !newEntries.find(oldEntry => matchFn(oldEntry, entry)))) {\n        newEntries.unshift(entry);\n      }\n    }\n\n    const newIndex = '\\n' + newEntries.map(entry => {\n      const stringified = JSON.stringify(entry);\n      const hash = hashEntry(stringified);\n      return `${hash}\\t${stringified}`;\n    }).join('\\n');\n\n    const setup = /*#__PURE__*/function () {\n      var _ref5 = _asyncToGenerator(function* () {\n        const target = uniqueFilename(path.join(cache, 'tmp'), opts.tmpPrefix);\n        yield fixOwner.mkdirfix(cache, path.dirname(target));\n        return {\n          target,\n          moved: false\n        };\n      });\n\n      return function setup() {\n        return _ref5.apply(this, arguments);\n      };\n    }();\n\n    const teardown = /*#__PURE__*/function () {\n      var _ref6 = _asyncToGenerator(function* (tmp) {\n        if (!tmp.moved) {\n          return rimraf(tmp.target);\n        }\n      });\n\n      return function teardown(_x15) {\n        return _ref6.apply(this, arguments);\n      };\n    }();\n\n    const write = /*#__PURE__*/function () {\n      var _ref7 = _asyncToGenerator(function* (tmp) {\n        yield fs.writeFile(tmp.target, newIndex, {\n          flag: 'wx'\n        });\n        yield fixOwner.mkdirfix(cache, path.dirname(bucket)); // we use @npmcli/move-file directly here because we\n        // want to overwrite the existing file\n\n        yield moveFile(tmp.target, bucket);\n        tmp.moved = true;\n\n        try {\n          yield fixOwner.chownr(cache, bucket);\n        } catch (err) {\n          if (err.code !== 'ENOENT') {\n            throw err;\n          }\n        }\n      });\n\n      return function write(_x16) {\n        return _ref7.apply(this, arguments);\n      };\n    }(); // write the file atomically\n\n\n    const tmp = yield setup();\n\n    try {\n      yield write(tmp);\n    } finally {\n      yield teardown(tmp);\n    } // we reverse the list we generated such that the newest\n    // entries come first in order to make looping through them easier\n    // the true passed to formatEntry tells it to keep null\n    // integrity values, if they made it this far it's because\n    // validateEntry returned true, and as such we should return it\n\n\n    return newEntries.reverse().map(entry => formatEntry(cache, entry, true));\n  });\n  return _compact.apply(this, arguments);\n}\n\nmodule.exports.insert = insert;\n\nfunction insert(_x4, _x5, _x6) {\n  return _insert.apply(this, arguments);\n}\n\nfunction _insert() {\n  _insert = _asyncToGenerator(function* (cache, key, integrity, opts = {}) {\n    const {\n      metadata,\n      size\n    } = opts;\n    const bucket = bucketPath(cache, key);\n    const entry = {\n      key,\n      integrity: integrity && ssri.stringify(integrity),\n      time: Date.now(),\n      size,\n      metadata\n    };\n\n    try {\n      yield fixOwner.mkdirfix(cache, path.dirname(bucket));\n      const stringified = JSON.stringify(entry); // NOTE - Cleverness ahoy!\n      //\n      // This works because it's tremendously unlikely for an entry to corrupt\n      // another while still preserving the string length of the JSON in\n      // question. So, we just slap the length in there and verify it on read.\n      //\n      // Thanks to @isaacs for the whiteboarding session that ended up with\n      // this.\n\n      yield fs.appendFile(bucket, `\\n${hashEntry(stringified)}\\t${stringified}`);\n      yield fixOwner.chownr(cache, bucket);\n    } catch (err) {\n      if (err.code === 'ENOENT') {\n        return undefined;\n      }\n\n      throw err; // There's a class of race conditions that happen when things get deleted\n      // during fixOwner, or between the two mkdirfix/chownr calls.\n      //\n      // It's perfectly fine to just not bother in those cases and lie\n      // that the index entry was written. Because it's a cache.\n    }\n\n    return formatEntry(cache, entry);\n  });\n  return _insert.apply(this, arguments);\n}\n\nmodule.exports.insert.sync = insertSync;\n\nfunction insertSync(cache, key, integrity, opts = {}) {\n  const {\n    metadata,\n    size\n  } = opts;\n  const bucket = bucketPath(cache, key);\n  const entry = {\n    key,\n    integrity: integrity && ssri.stringify(integrity),\n    time: Date.now(),\n    size,\n    metadata\n  };\n  fixOwner.mkdirfix.sync(cache, path.dirname(bucket));\n  const stringified = JSON.stringify(entry);\n  fs.appendFileSync(bucket, `\\n${hashEntry(stringified)}\\t${stringified}`);\n\n  try {\n    fixOwner.chownr.sync(cache, bucket);\n  } catch (err) {\n    if (err.code !== 'ENOENT') {\n      throw err;\n    }\n  }\n\n  return formatEntry(cache, entry);\n}\n\nmodule.exports.find = find;\n\nfunction find(_x7, _x8) {\n  return _find.apply(this, arguments);\n}\n\nfunction _find() {\n  _find = _asyncToGenerator(function* (cache, key) {\n    const bucket = bucketPath(cache, key);\n\n    try {\n      const entries = yield bucketEntries(bucket);\n      return entries.reduce((latest, next) => {\n        if (next && next.key === key) {\n          return formatEntry(cache, next);\n        } else {\n          return latest;\n        }\n      }, null);\n    } catch (err) {\n      if (err.code === 'ENOENT') {\n        return null;\n      } else {\n        throw err;\n      }\n    }\n  });\n  return _find.apply(this, arguments);\n}\n\nmodule.exports.find.sync = findSync;\n\nfunction findSync(cache, key) {\n  const bucket = bucketPath(cache, key);\n\n  try {\n    return bucketEntriesSync(bucket).reduce((latest, next) => {\n      if (next && next.key === key) {\n        return formatEntry(cache, next);\n      } else {\n        return latest;\n      }\n    }, null);\n  } catch (err) {\n    if (err.code === 'ENOENT') {\n      return null;\n    } else {\n      throw err;\n    }\n  }\n}\n\nmodule.exports.delete = del;\n\nfunction del(cache, key, opts = {}) {\n  if (!opts.removeFully) {\n    return insert(cache, key, null, opts);\n  }\n\n  const bucket = bucketPath(cache, key);\n  return rimraf(bucket);\n}\n\nmodule.exports.delete.sync = delSync;\n\nfunction delSync(cache, key, opts = {}) {\n  if (!opts.removeFully) {\n    return insertSync(cache, key, null, opts);\n  }\n\n  const bucket = bucketPath(cache, key);\n  return rimraf.sync(bucket);\n}\n\nmodule.exports.lsStream = lsStream;\n\nfunction lsStream(cache) {\n  const indexDir = bucketDir(cache);\n  const stream = new Minipass({\n    objectMode: true\n  }); // Set all this up to run on the stream and then just return the stream\n\n  Promise.resolve().then( /*#__PURE__*/_asyncToGenerator(function* () {\n    const buckets = yield readdirOrEmpty(indexDir);\n    yield Promise.all(buckets.map( /*#__PURE__*/function () {\n      var _ref2 = _asyncToGenerator(function* (bucket) {\n        const bucketPath = path.join(indexDir, bucket);\n        const subbuckets = yield readdirOrEmpty(bucketPath);\n        yield Promise.all(subbuckets.map( /*#__PURE__*/function () {\n          var _ref3 = _asyncToGenerator(function* (subbucket) {\n            const subbucketPath = path.join(bucketPath, subbucket); // \"/cachename/<bucket 0xFF>/<bucket 0xFF>./*\"\n\n            const subbucketEntries = yield readdirOrEmpty(subbucketPath);\n            yield Promise.all(subbucketEntries.map( /*#__PURE__*/function () {\n              var _ref4 = _asyncToGenerator(function* (entry) {\n                const entryPath = path.join(subbucketPath, entry);\n\n                try {\n                  const entries = yield bucketEntries(entryPath); // using a Map here prevents duplicate keys from showing up\n                  // twice, I guess?\n\n                  const reduced = entries.reduce((acc, entry) => {\n                    acc.set(entry.key, entry);\n                    return acc;\n                  }, new Map()); // reduced is a map of key => entry\n\n                  for (const entry of reduced.values()) {\n                    const formatted = formatEntry(cache, entry);\n\n                    if (formatted) {\n                      stream.write(formatted);\n                    }\n                  }\n                } catch (err) {\n                  if (err.code === 'ENOENT') {\n                    return undefined;\n                  }\n\n                  throw err;\n                }\n              });\n\n              return function (_x11) {\n                return _ref4.apply(this, arguments);\n              };\n            }()));\n          });\n\n          return function (_x10) {\n            return _ref3.apply(this, arguments);\n          };\n        }()));\n      });\n\n      return function (_x9) {\n        return _ref2.apply(this, arguments);\n      };\n    }()));\n    stream.end();\n    return stream;\n  })).catch(err => stream.emit('error', err));\n  return stream;\n}\n\nmodule.exports.ls = ls;\n\nfunction ls(_x12) {\n  return _ls.apply(this, arguments);\n}\n\nfunction _ls() {\n  _ls = _asyncToGenerator(function* (cache) {\n    const entries = yield lsStream(cache).collect();\n    return entries.reduce((acc, xs) => {\n      acc[xs.key] = xs;\n      return acc;\n    }, {});\n  });\n  return _ls.apply(this, arguments);\n}\n\nmodule.exports.bucketEntries = bucketEntries;\n\nfunction bucketEntries(_x13, _x14) {\n  return _bucketEntries2.apply(this, arguments);\n}\n\nfunction _bucketEntries2() {\n  _bucketEntries2 = _asyncToGenerator(function* (bucket, filter) {\n    const data = yield fs.readFile(bucket, 'utf8');\n    return _bucketEntries(data, filter);\n  });\n  return _bucketEntries2.apply(this, arguments);\n}\n\nmodule.exports.bucketEntries.sync = bucketEntriesSync;\n\nfunction bucketEntriesSync(bucket, filter) {\n  const data = fs.readFileSync(bucket, 'utf8');\n  return _bucketEntries(data, filter);\n}\n\nfunction _bucketEntries(data, filter) {\n  const entries = [];\n  data.split('\\n').forEach(entry => {\n    if (!entry) {\n      return;\n    }\n\n    const pieces = entry.split('\\t');\n\n    if (!pieces[1] || hashEntry(pieces[1]) !== pieces[0]) {\n      // Hash is no good! Corruption or malice? Doesn't matter!\n      // EJECT EJECT\n      return;\n    }\n\n    let obj;\n\n    try {\n      obj = JSON.parse(pieces[1]);\n    } catch (e) {\n      // Entry is corrupted!\n      return;\n    }\n\n    if (obj) {\n      entries.push(obj);\n    }\n  });\n  return entries;\n}\n\nmodule.exports.bucketDir = bucketDir;\n\nfunction bucketDir(cache) {\n  return path.join(cache, `index-v${indexV}`);\n}\n\nmodule.exports.bucketPath = bucketPath;\n\nfunction bucketPath(cache, key) {\n  const hashed = hashKey(key);\n  return path.join.apply(path, [bucketDir(cache)].concat(hashToSegments(hashed)));\n}\n\nmodule.exports.hashKey = hashKey;\n\nfunction hashKey(key) {\n  return hash(key, 'sha256');\n}\n\nmodule.exports.hashEntry = hashEntry;\n\nfunction hashEntry(str) {\n  return hash(str, 'sha1');\n}\n\nfunction hash(str, digest) {\n  return crypto.createHash(digest).update(str).digest('hex');\n}\n\nfunction formatEntry(cache, entry, keepAll) {\n  // Treat null digests as deletions. They'll shadow any previous entries.\n  if (!entry.integrity && !keepAll) {\n    return null;\n  }\n\n  return {\n    key: entry.key,\n    integrity: entry.integrity,\n    path: entry.integrity ? contentPath(cache, entry.integrity) : undefined,\n    size: entry.size,\n    time: entry.time,\n    metadata: entry.metadata\n  };\n}\n\nfunction readdirOrEmpty(dir) {\n  return fs.readdir(dir).catch(err => {\n    if (err.code === 'ENOENT' || err.code === 'ENOTDIR') {\n      return [];\n    }\n\n    throw err;\n  });\n}","map":{"version":3,"names":["util","require","crypto","fs","Minipass","path","ssri","uniqueFilename","contentPath","fixOwner","hashToSegments","indexV","index","moveFile","_rimraf","rimraf","promisify","sync","module","exports","NotFoundError","Error","constructor","cache","key","code","compact","matchFn","opts","bucket","bucketPath","entries","bucketEntries","newEntries","i","length","entry","integrity","validateEntry","find","oldEntry","unshift","newIndex","map","stringified","JSON","stringify","hash","hashEntry","join","setup","target","tmpPrefix","mkdirfix","dirname","moved","teardown","tmp","write","writeFile","flag","chownr","err","reverse","formatEntry","insert","metadata","size","time","Date","now","appendFile","undefined","insertSync","appendFileSync","reduce","latest","next","findSync","bucketEntriesSync","delete","del","removeFully","delSync","lsStream","indexDir","bucketDir","stream","objectMode","Promise","resolve","then","buckets","readdirOrEmpty","all","subbuckets","subbucket","subbucketPath","subbucketEntries","entryPath","reduced","acc","set","Map","values","formatted","end","catch","emit","ls","collect","xs","filter","data","readFile","_bucketEntries","readFileSync","split","forEach","pieces","obj","parse","e","push","hashed","hashKey","apply","concat","str","digest","createHash","update","keepAll","dir","readdir"],"sources":["C:/sources/GitHub/appBlog/BlogLab/BlogLab-UI/node_modules/cacache/lib/entry-index.js"],"sourcesContent":["'use strict'\n\nconst util = require('util')\nconst crypto = require('crypto')\nconst fs = require('@npmcli/fs')\nconst Minipass = require('minipass')\nconst path = require('path')\nconst ssri = require('ssri')\nconst uniqueFilename = require('unique-filename')\n\nconst contentPath = require('./content/path')\nconst fixOwner = require('./util/fix-owner')\nconst hashToSegments = require('./util/hash-to-segments')\nconst indexV = require('../package.json')['cache-version'].index\nconst moveFile = require('@npmcli/move-file')\nconst _rimraf = require('rimraf')\nconst rimraf = util.promisify(_rimraf)\nrimraf.sync = _rimraf.sync\n\nmodule.exports.NotFoundError = class NotFoundError extends Error {\n  constructor (cache, key) {\n    super(`No cache entry for ${key} found in ${cache}`)\n    this.code = 'ENOENT'\n    this.cache = cache\n    this.key = key\n  }\n}\n\nmodule.exports.compact = compact\n\nasync function compact (cache, key, matchFn, opts = {}) {\n  const bucket = bucketPath(cache, key)\n  const entries = await bucketEntries(bucket)\n  const newEntries = []\n  // we loop backwards because the bottom-most result is the newest\n  // since we add new entries with appendFile\n  for (let i = entries.length - 1; i >= 0; --i) {\n    const entry = entries[i]\n    // a null integrity could mean either a delete was appended\n    // or the user has simply stored an index that does not map\n    // to any content. we determine if the user wants to keep the\n    // null integrity based on the validateEntry function passed in options.\n    // if the integrity is null and no validateEntry is provided, we break\n    // as we consider the null integrity to be a deletion of everything\n    // that came before it.\n    if (entry.integrity === null && !opts.validateEntry) {\n      break\n    }\n\n    // if this entry is valid, and it is either the first entry or\n    // the newEntries array doesn't already include an entry that\n    // matches this one based on the provided matchFn, then we add\n    // it to the beginning of our list\n    if ((!opts.validateEntry || opts.validateEntry(entry) === true) &&\n      (newEntries.length === 0 ||\n        !newEntries.find((oldEntry) => matchFn(oldEntry, entry)))) {\n      newEntries.unshift(entry)\n    }\n  }\n\n  const newIndex = '\\n' + newEntries.map((entry) => {\n    const stringified = JSON.stringify(entry)\n    const hash = hashEntry(stringified)\n    return `${hash}\\t${stringified}`\n  }).join('\\n')\n\n  const setup = async () => {\n    const target = uniqueFilename(path.join(cache, 'tmp'), opts.tmpPrefix)\n    await fixOwner.mkdirfix(cache, path.dirname(target))\n    return {\n      target,\n      moved: false,\n    }\n  }\n\n  const teardown = async (tmp) => {\n    if (!tmp.moved) {\n      return rimraf(tmp.target)\n    }\n  }\n\n  const write = async (tmp) => {\n    await fs.writeFile(tmp.target, newIndex, { flag: 'wx' })\n    await fixOwner.mkdirfix(cache, path.dirname(bucket))\n    // we use @npmcli/move-file directly here because we\n    // want to overwrite the existing file\n    await moveFile(tmp.target, bucket)\n    tmp.moved = true\n    try {\n      await fixOwner.chownr(cache, bucket)\n    } catch (err) {\n      if (err.code !== 'ENOENT') {\n        throw err\n      }\n    }\n  }\n\n  // write the file atomically\n  const tmp = await setup()\n  try {\n    await write(tmp)\n  } finally {\n    await teardown(tmp)\n  }\n\n  // we reverse the list we generated such that the newest\n  // entries come first in order to make looping through them easier\n  // the true passed to formatEntry tells it to keep null\n  // integrity values, if they made it this far it's because\n  // validateEntry returned true, and as such we should return it\n  return newEntries.reverse().map((entry) => formatEntry(cache, entry, true))\n}\n\nmodule.exports.insert = insert\n\nasync function insert (cache, key, integrity, opts = {}) {\n  const { metadata, size } = opts\n  const bucket = bucketPath(cache, key)\n  const entry = {\n    key,\n    integrity: integrity && ssri.stringify(integrity),\n    time: Date.now(),\n    size,\n    metadata,\n  }\n  try {\n    await fixOwner.mkdirfix(cache, path.dirname(bucket))\n    const stringified = JSON.stringify(entry)\n    // NOTE - Cleverness ahoy!\n    //\n    // This works because it's tremendously unlikely for an entry to corrupt\n    // another while still preserving the string length of the JSON in\n    // question. So, we just slap the length in there and verify it on read.\n    //\n    // Thanks to @isaacs for the whiteboarding session that ended up with\n    // this.\n    await fs.appendFile(bucket, `\\n${hashEntry(stringified)}\\t${stringified}`)\n    await fixOwner.chownr(cache, bucket)\n  } catch (err) {\n    if (err.code === 'ENOENT') {\n      return undefined\n    }\n\n    throw err\n    // There's a class of race conditions that happen when things get deleted\n    // during fixOwner, or between the two mkdirfix/chownr calls.\n    //\n    // It's perfectly fine to just not bother in those cases and lie\n    // that the index entry was written. Because it's a cache.\n  }\n  return formatEntry(cache, entry)\n}\n\nmodule.exports.insert.sync = insertSync\n\nfunction insertSync (cache, key, integrity, opts = {}) {\n  const { metadata, size } = opts\n  const bucket = bucketPath(cache, key)\n  const entry = {\n    key,\n    integrity: integrity && ssri.stringify(integrity),\n    time: Date.now(),\n    size,\n    metadata,\n  }\n  fixOwner.mkdirfix.sync(cache, path.dirname(bucket))\n  const stringified = JSON.stringify(entry)\n  fs.appendFileSync(bucket, `\\n${hashEntry(stringified)}\\t${stringified}`)\n  try {\n    fixOwner.chownr.sync(cache, bucket)\n  } catch (err) {\n    if (err.code !== 'ENOENT') {\n      throw err\n    }\n  }\n  return formatEntry(cache, entry)\n}\n\nmodule.exports.find = find\n\nasync function find (cache, key) {\n  const bucket = bucketPath(cache, key)\n  try {\n    const entries = await bucketEntries(bucket)\n    return entries.reduce((latest, next) => {\n      if (next && next.key === key) {\n        return formatEntry(cache, next)\n      } else {\n        return latest\n      }\n    }, null)\n  } catch (err) {\n    if (err.code === 'ENOENT') {\n      return null\n    } else {\n      throw err\n    }\n  }\n}\n\nmodule.exports.find.sync = findSync\n\nfunction findSync (cache, key) {\n  const bucket = bucketPath(cache, key)\n  try {\n    return bucketEntriesSync(bucket).reduce((latest, next) => {\n      if (next && next.key === key) {\n        return formatEntry(cache, next)\n      } else {\n        return latest\n      }\n    }, null)\n  } catch (err) {\n    if (err.code === 'ENOENT') {\n      return null\n    } else {\n      throw err\n    }\n  }\n}\n\nmodule.exports.delete = del\n\nfunction del (cache, key, opts = {}) {\n  if (!opts.removeFully) {\n    return insert(cache, key, null, opts)\n  }\n\n  const bucket = bucketPath(cache, key)\n  return rimraf(bucket)\n}\n\nmodule.exports.delete.sync = delSync\n\nfunction delSync (cache, key, opts = {}) {\n  if (!opts.removeFully) {\n    return insertSync(cache, key, null, opts)\n  }\n\n  const bucket = bucketPath(cache, key)\n  return rimraf.sync(bucket)\n}\n\nmodule.exports.lsStream = lsStream\n\nfunction lsStream (cache) {\n  const indexDir = bucketDir(cache)\n  const stream = new Minipass({ objectMode: true })\n\n  // Set all this up to run on the stream and then just return the stream\n  Promise.resolve().then(async () => {\n    const buckets = await readdirOrEmpty(indexDir)\n    await Promise.all(buckets.map(async (bucket) => {\n      const bucketPath = path.join(indexDir, bucket)\n      const subbuckets = await readdirOrEmpty(bucketPath)\n      await Promise.all(subbuckets.map(async (subbucket) => {\n        const subbucketPath = path.join(bucketPath, subbucket)\n\n        // \"/cachename/<bucket 0xFF>/<bucket 0xFF>./*\"\n        const subbucketEntries = await readdirOrEmpty(subbucketPath)\n        await Promise.all(subbucketEntries.map(async (entry) => {\n          const entryPath = path.join(subbucketPath, entry)\n          try {\n            const entries = await bucketEntries(entryPath)\n            // using a Map here prevents duplicate keys from showing up\n            // twice, I guess?\n            const reduced = entries.reduce((acc, entry) => {\n              acc.set(entry.key, entry)\n              return acc\n            }, new Map())\n            // reduced is a map of key => entry\n            for (const entry of reduced.values()) {\n              const formatted = formatEntry(cache, entry)\n              if (formatted) {\n                stream.write(formatted)\n              }\n            }\n          } catch (err) {\n            if (err.code === 'ENOENT') {\n              return undefined\n            }\n            throw err\n          }\n        }))\n      }))\n    }))\n    stream.end()\n    return stream\n  }).catch(err => stream.emit('error', err))\n\n  return stream\n}\n\nmodule.exports.ls = ls\n\nasync function ls (cache) {\n  const entries = await lsStream(cache).collect()\n  return entries.reduce((acc, xs) => {\n    acc[xs.key] = xs\n    return acc\n  }, {})\n}\n\nmodule.exports.bucketEntries = bucketEntries\n\nasync function bucketEntries (bucket, filter) {\n  const data = await fs.readFile(bucket, 'utf8')\n  return _bucketEntries(data, filter)\n}\n\nmodule.exports.bucketEntries.sync = bucketEntriesSync\n\nfunction bucketEntriesSync (bucket, filter) {\n  const data = fs.readFileSync(bucket, 'utf8')\n  return _bucketEntries(data, filter)\n}\n\nfunction _bucketEntries (data, filter) {\n  const entries = []\n  data.split('\\n').forEach((entry) => {\n    if (!entry) {\n      return\n    }\n\n    const pieces = entry.split('\\t')\n    if (!pieces[1] || hashEntry(pieces[1]) !== pieces[0]) {\n      // Hash is no good! Corruption or malice? Doesn't matter!\n      // EJECT EJECT\n      return\n    }\n    let obj\n    try {\n      obj = JSON.parse(pieces[1])\n    } catch (e) {\n      // Entry is corrupted!\n      return\n    }\n    if (obj) {\n      entries.push(obj)\n    }\n  })\n  return entries\n}\n\nmodule.exports.bucketDir = bucketDir\n\nfunction bucketDir (cache) {\n  return path.join(cache, `index-v${indexV}`)\n}\n\nmodule.exports.bucketPath = bucketPath\n\nfunction bucketPath (cache, key) {\n  const hashed = hashKey(key)\n  return path.join.apply(\n    path,\n    [bucketDir(cache)].concat(hashToSegments(hashed))\n  )\n}\n\nmodule.exports.hashKey = hashKey\n\nfunction hashKey (key) {\n  return hash(key, 'sha256')\n}\n\nmodule.exports.hashEntry = hashEntry\n\nfunction hashEntry (str) {\n  return hash(str, 'sha1')\n}\n\nfunction hash (str, digest) {\n  return crypto\n    .createHash(digest)\n    .update(str)\n    .digest('hex')\n}\n\nfunction formatEntry (cache, entry, keepAll) {\n  // Treat null digests as deletions. They'll shadow any previous entries.\n  if (!entry.integrity && !keepAll) {\n    return null\n  }\n\n  return {\n    key: entry.key,\n    integrity: entry.integrity,\n    path: entry.integrity ? contentPath(cache, entry.integrity) : undefined,\n    size: entry.size,\n    time: entry.time,\n    metadata: entry.metadata,\n  }\n}\n\nfunction readdirOrEmpty (dir) {\n  return fs.readdir(dir).catch((err) => {\n    if (err.code === 'ENOENT' || err.code === 'ENOTDIR') {\n      return []\n    }\n\n    throw err\n  })\n}\n"],"mappings":"AAAA;;;;AAEA,MAAMA,IAAI,GAAGC,OAAO,CAAC,MAAD,CAApB;;AACA,MAAMC,MAAM,GAAGD,OAAO,CAAC,QAAD,CAAtB;;AACA,MAAME,EAAE,GAAGF,OAAO,CAAC,YAAD,CAAlB;;AACA,MAAMG,QAAQ,GAAGH,OAAO,CAAC,UAAD,CAAxB;;AACA,MAAMI,IAAI,GAAGJ,OAAO,CAAC,MAAD,CAApB;;AACA,MAAMK,IAAI,GAAGL,OAAO,CAAC,MAAD,CAApB;;AACA,MAAMM,cAAc,GAAGN,OAAO,CAAC,iBAAD,CAA9B;;AAEA,MAAMO,WAAW,GAAGP,OAAO,CAAC,gBAAD,CAA3B;;AACA,MAAMQ,QAAQ,GAAGR,OAAO,CAAC,kBAAD,CAAxB;;AACA,MAAMS,cAAc,GAAGT,OAAO,CAAC,yBAAD,CAA9B;;AACA,MAAMU,MAAM,GAAGV,OAAO,CAAC,iBAAD,CAAP,CAA2B,eAA3B,EAA4CW,KAA3D;;AACA,MAAMC,QAAQ,GAAGZ,OAAO,CAAC,mBAAD,CAAxB;;AACA,MAAMa,OAAO,GAAGb,OAAO,CAAC,QAAD,CAAvB;;AACA,MAAMc,MAAM,GAAGf,IAAI,CAACgB,SAAL,CAAeF,OAAf,CAAf;AACAC,MAAM,CAACE,IAAP,GAAcH,OAAO,CAACG,IAAtB;AAEAC,MAAM,CAACC,OAAP,CAAeC,aAAf,GAA+B,MAAMA,aAAN,SAA4BC,KAA5B,CAAkC;EAC/DC,WAAW,CAAEC,KAAF,EAASC,GAAT,EAAc;IACvB,MAAO,sBAAqBA,GAAI,aAAYD,KAAM,EAAlD;IACA,KAAKE,IAAL,GAAY,QAAZ;IACA,KAAKF,KAAL,GAAaA,KAAb;IACA,KAAKC,GAAL,GAAWA,GAAX;EACD;;AAN8D,CAAjE;AASAN,MAAM,CAACC,OAAP,CAAeO,OAAf,GAAyBA,OAAzB;;SAEeA,O;;;;;+BAAf,WAAwBH,KAAxB,EAA+BC,GAA/B,EAAoCG,OAApC,EAA6CC,IAAI,GAAG,EAApD,EAAwD;IACtD,MAAMC,MAAM,GAAGC,UAAU,CAACP,KAAD,EAAQC,GAAR,CAAzB;IACA,MAAMO,OAAO,SAASC,aAAa,CAACH,MAAD,CAAnC;IACA,MAAMI,UAAU,GAAG,EAAnB,CAHsD,CAItD;IACA;;IACA,KAAK,IAAIC,CAAC,GAAGH,OAAO,CAACI,MAAR,GAAiB,CAA9B,EAAiCD,CAAC,IAAI,CAAtC,EAAyC,EAAEA,CAA3C,EAA8C;MAC5C,MAAME,KAAK,GAAGL,OAAO,CAACG,CAAD,CAArB,CAD4C,CAE5C;MACA;MACA;MACA;MACA;MACA;MACA;;MACA,IAAIE,KAAK,CAACC,SAAN,KAAoB,IAApB,IAA4B,CAACT,IAAI,CAACU,aAAtC,EAAqD;QACnD;MACD,CAX2C,CAa5C;MACA;MACA;MACA;;;MACA,IAAI,CAAC,CAACV,IAAI,CAACU,aAAN,IAAuBV,IAAI,CAACU,aAAL,CAAmBF,KAAnB,MAA8B,IAAtD,MACDH,UAAU,CAACE,MAAX,KAAsB,CAAtB,IACC,CAACF,UAAU,CAACM,IAAX,CAAiBC,QAAD,IAAcb,OAAO,CAACa,QAAD,EAAWJ,KAAX,CAArC,CAFD,CAAJ,EAE+D;QAC7DH,UAAU,CAACQ,OAAX,CAAmBL,KAAnB;MACD;IACF;;IAED,MAAMM,QAAQ,GAAG,OAAOT,UAAU,CAACU,GAAX,CAAgBP,KAAD,IAAW;MAChD,MAAMQ,WAAW,GAAGC,IAAI,CAACC,SAAL,CAAeV,KAAf,CAApB;MACA,MAAMW,IAAI,GAAGC,SAAS,CAACJ,WAAD,CAAtB;MACA,OAAQ,GAAEG,IAAK,KAAIH,WAAY,EAA/B;IACD,CAJuB,EAIrBK,IAJqB,CAIhB,IAJgB,CAAxB;;IAMA,MAAMC,KAAK;MAAA,8BAAG,aAAY;QACxB,MAAMC,MAAM,GAAG5C,cAAc,CAACF,IAAI,CAAC4C,IAAL,CAAU1B,KAAV,EAAiB,KAAjB,CAAD,EAA0BK,IAAI,CAACwB,SAA/B,CAA7B;QACA,MAAM3C,QAAQ,CAAC4C,QAAT,CAAkB9B,KAAlB,EAAyBlB,IAAI,CAACiD,OAAL,CAAaH,MAAb,CAAzB,CAAN;QACA,OAAO;UACLA,MADK;UAELI,KAAK,EAAE;QAFF,CAAP;MAID,CAPU;;MAAA,gBAALL,KAAK;QAAA;MAAA;IAAA,GAAX;;IASA,MAAMM,QAAQ;MAAA,8BAAG,WAAOC,GAAP,EAAe;QAC9B,IAAI,CAACA,GAAG,CAACF,KAAT,EAAgB;UACd,OAAOxC,MAAM,CAAC0C,GAAG,CAACN,MAAL,CAAb;QACD;MACF,CAJa;;MAAA,gBAARK,QAAQ;QAAA;MAAA;IAAA,GAAd;;IAMA,MAAME,KAAK;MAAA,8BAAG,WAAOD,GAAP,EAAe;QAC3B,MAAMtD,EAAE,CAACwD,SAAH,CAAaF,GAAG,CAACN,MAAjB,EAAyBT,QAAzB,EAAmC;UAAEkB,IAAI,EAAE;QAAR,CAAnC,CAAN;QACA,MAAMnD,QAAQ,CAAC4C,QAAT,CAAkB9B,KAAlB,EAAyBlB,IAAI,CAACiD,OAAL,CAAazB,MAAb,CAAzB,CAAN,CAF2B,CAG3B;QACA;;QACA,MAAMhB,QAAQ,CAAC4C,GAAG,CAACN,MAAL,EAAatB,MAAb,CAAd;QACA4B,GAAG,CAACF,KAAJ,GAAY,IAAZ;;QACA,IAAI;UACF,MAAM9C,QAAQ,CAACoD,MAAT,CAAgBtC,KAAhB,EAAuBM,MAAvB,CAAN;QACD,CAFD,CAEE,OAAOiC,GAAP,EAAY;UACZ,IAAIA,GAAG,CAACrC,IAAJ,KAAa,QAAjB,EAA2B;YACzB,MAAMqC,GAAN;UACD;QACF;MACF,CAdU;;MAAA,gBAALJ,KAAK;QAAA;MAAA;IAAA,GAAX,CAnDsD,CAmEtD;;;IACA,MAAMD,GAAG,SAASP,KAAK,EAAvB;;IACA,IAAI;MACF,MAAMQ,KAAK,CAACD,GAAD,CAAX;IACD,CAFD,SAEU;MACR,MAAMD,QAAQ,CAACC,GAAD,CAAd;IACD,CAzEqD,CA2EtD;IACA;IACA;IACA;IACA;;;IACA,OAAOxB,UAAU,CAAC8B,OAAX,GAAqBpB,GAArB,CAA0BP,KAAD,IAAW4B,WAAW,CAACzC,KAAD,EAAQa,KAAR,EAAe,IAAf,CAA/C,CAAP;EACD,C;;;;AAEDlB,MAAM,CAACC,OAAP,CAAe8C,MAAf,GAAwBA,MAAxB;;SAEeA,M;;;;;8BAAf,WAAuB1C,KAAvB,EAA8BC,GAA9B,EAAmCa,SAAnC,EAA8CT,IAAI,GAAG,EAArD,EAAyD;IACvD,MAAM;MAAEsC,QAAF;MAAYC;IAAZ,IAAqBvC,IAA3B;IACA,MAAMC,MAAM,GAAGC,UAAU,CAACP,KAAD,EAAQC,GAAR,CAAzB;IACA,MAAMY,KAAK,GAAG;MACZZ,GADY;MAEZa,SAAS,EAAEA,SAAS,IAAI/B,IAAI,CAACwC,SAAL,CAAeT,SAAf,CAFZ;MAGZ+B,IAAI,EAAEC,IAAI,CAACC,GAAL,EAHM;MAIZH,IAJY;MAKZD;IALY,CAAd;;IAOA,IAAI;MACF,MAAMzD,QAAQ,CAAC4C,QAAT,CAAkB9B,KAAlB,EAAyBlB,IAAI,CAACiD,OAAL,CAAazB,MAAb,CAAzB,CAAN;MACA,MAAMe,WAAW,GAAGC,IAAI,CAACC,SAAL,CAAeV,KAAf,CAApB,CAFE,CAGF;MACA;MACA;MACA;MACA;MACA;MACA;MACA;;MACA,MAAMjC,EAAE,CAACoE,UAAH,CAAc1C,MAAd,EAAuB,KAAImB,SAAS,CAACJ,WAAD,CAAc,KAAIA,WAAY,EAAlE,CAAN;MACA,MAAMnC,QAAQ,CAACoD,MAAT,CAAgBtC,KAAhB,EAAuBM,MAAvB,CAAN;IACD,CAbD,CAaE,OAAOiC,GAAP,EAAY;MACZ,IAAIA,GAAG,CAACrC,IAAJ,KAAa,QAAjB,EAA2B;QACzB,OAAO+C,SAAP;MACD;;MAED,MAAMV,GAAN,CALY,CAMZ;MACA;MACA;MACA;MACA;IACD;;IACD,OAAOE,WAAW,CAACzC,KAAD,EAAQa,KAAR,CAAlB;EACD,C;;;;AAEDlB,MAAM,CAACC,OAAP,CAAe8C,MAAf,CAAsBhD,IAAtB,GAA6BwD,UAA7B;;AAEA,SAASA,UAAT,CAAqBlD,KAArB,EAA4BC,GAA5B,EAAiCa,SAAjC,EAA4CT,IAAI,GAAG,EAAnD,EAAuD;EACrD,MAAM;IAAEsC,QAAF;IAAYC;EAAZ,IAAqBvC,IAA3B;EACA,MAAMC,MAAM,GAAGC,UAAU,CAACP,KAAD,EAAQC,GAAR,CAAzB;EACA,MAAMY,KAAK,GAAG;IACZZ,GADY;IAEZa,SAAS,EAAEA,SAAS,IAAI/B,IAAI,CAACwC,SAAL,CAAeT,SAAf,CAFZ;IAGZ+B,IAAI,EAAEC,IAAI,CAACC,GAAL,EAHM;IAIZH,IAJY;IAKZD;EALY,CAAd;EAOAzD,QAAQ,CAAC4C,QAAT,CAAkBpC,IAAlB,CAAuBM,KAAvB,EAA8BlB,IAAI,CAACiD,OAAL,CAAazB,MAAb,CAA9B;EACA,MAAMe,WAAW,GAAGC,IAAI,CAACC,SAAL,CAAeV,KAAf,CAApB;EACAjC,EAAE,CAACuE,cAAH,CAAkB7C,MAAlB,EAA2B,KAAImB,SAAS,CAACJ,WAAD,CAAc,KAAIA,WAAY,EAAtE;;EACA,IAAI;IACFnC,QAAQ,CAACoD,MAAT,CAAgB5C,IAAhB,CAAqBM,KAArB,EAA4BM,MAA5B;EACD,CAFD,CAEE,OAAOiC,GAAP,EAAY;IACZ,IAAIA,GAAG,CAACrC,IAAJ,KAAa,QAAjB,EAA2B;MACzB,MAAMqC,GAAN;IACD;EACF;;EACD,OAAOE,WAAW,CAACzC,KAAD,EAAQa,KAAR,CAAlB;AACD;;AAEDlB,MAAM,CAACC,OAAP,CAAeoB,IAAf,GAAsBA,IAAtB;;SAEeA,I;;;;;4BAAf,WAAqBhB,KAArB,EAA4BC,GAA5B,EAAiC;IAC/B,MAAMK,MAAM,GAAGC,UAAU,CAACP,KAAD,EAAQC,GAAR,CAAzB;;IACA,IAAI;MACF,MAAMO,OAAO,SAASC,aAAa,CAACH,MAAD,CAAnC;MACA,OAAOE,OAAO,CAAC4C,MAAR,CAAe,CAACC,MAAD,EAASC,IAAT,KAAkB;QACtC,IAAIA,IAAI,IAAIA,IAAI,CAACrD,GAAL,KAAaA,GAAzB,EAA8B;UAC5B,OAAOwC,WAAW,CAACzC,KAAD,EAAQsD,IAAR,CAAlB;QACD,CAFD,MAEO;UACL,OAAOD,MAAP;QACD;MACF,CANM,EAMJ,IANI,CAAP;IAOD,CATD,CASE,OAAOd,GAAP,EAAY;MACZ,IAAIA,GAAG,CAACrC,IAAJ,KAAa,QAAjB,EAA2B;QACzB,OAAO,IAAP;MACD,CAFD,MAEO;QACL,MAAMqC,GAAN;MACD;IACF;EACF,C;;;;AAED5C,MAAM,CAACC,OAAP,CAAeoB,IAAf,CAAoBtB,IAApB,GAA2B6D,QAA3B;;AAEA,SAASA,QAAT,CAAmBvD,KAAnB,EAA0BC,GAA1B,EAA+B;EAC7B,MAAMK,MAAM,GAAGC,UAAU,CAACP,KAAD,EAAQC,GAAR,CAAzB;;EACA,IAAI;IACF,OAAOuD,iBAAiB,CAAClD,MAAD,CAAjB,CAA0B8C,MAA1B,CAAiC,CAACC,MAAD,EAASC,IAAT,KAAkB;MACxD,IAAIA,IAAI,IAAIA,IAAI,CAACrD,GAAL,KAAaA,GAAzB,EAA8B;QAC5B,OAAOwC,WAAW,CAACzC,KAAD,EAAQsD,IAAR,CAAlB;MACD,CAFD,MAEO;QACL,OAAOD,MAAP;MACD;IACF,CANM,EAMJ,IANI,CAAP;EAOD,CARD,CAQE,OAAOd,GAAP,EAAY;IACZ,IAAIA,GAAG,CAACrC,IAAJ,KAAa,QAAjB,EAA2B;MACzB,OAAO,IAAP;IACD,CAFD,MAEO;MACL,MAAMqC,GAAN;IACD;EACF;AACF;;AAED5C,MAAM,CAACC,OAAP,CAAe6D,MAAf,GAAwBC,GAAxB;;AAEA,SAASA,GAAT,CAAc1D,KAAd,EAAqBC,GAArB,EAA0BI,IAAI,GAAG,EAAjC,EAAqC;EACnC,IAAI,CAACA,IAAI,CAACsD,WAAV,EAAuB;IACrB,OAAOjB,MAAM,CAAC1C,KAAD,EAAQC,GAAR,EAAa,IAAb,EAAmBI,IAAnB,CAAb;EACD;;EAED,MAAMC,MAAM,GAAGC,UAAU,CAACP,KAAD,EAAQC,GAAR,CAAzB;EACA,OAAOT,MAAM,CAACc,MAAD,CAAb;AACD;;AAEDX,MAAM,CAACC,OAAP,CAAe6D,MAAf,CAAsB/D,IAAtB,GAA6BkE,OAA7B;;AAEA,SAASA,OAAT,CAAkB5D,KAAlB,EAAyBC,GAAzB,EAA8BI,IAAI,GAAG,EAArC,EAAyC;EACvC,IAAI,CAACA,IAAI,CAACsD,WAAV,EAAuB;IACrB,OAAOT,UAAU,CAAClD,KAAD,EAAQC,GAAR,EAAa,IAAb,EAAmBI,IAAnB,CAAjB;EACD;;EAED,MAAMC,MAAM,GAAGC,UAAU,CAACP,KAAD,EAAQC,GAAR,CAAzB;EACA,OAAOT,MAAM,CAACE,IAAP,CAAYY,MAAZ,CAAP;AACD;;AAEDX,MAAM,CAACC,OAAP,CAAeiE,QAAf,GAA0BA,QAA1B;;AAEA,SAASA,QAAT,CAAmB7D,KAAnB,EAA0B;EACxB,MAAM8D,QAAQ,GAAGC,SAAS,CAAC/D,KAAD,CAA1B;EACA,MAAMgE,MAAM,GAAG,IAAInF,QAAJ,CAAa;IAAEoF,UAAU,EAAE;EAAd,CAAb,CAAf,CAFwB,CAIxB;;EACAC,OAAO,CAACC,OAAR,GAAkBC,IAAlB,iCAAuB,aAAY;IACjC,MAAMC,OAAO,SAASC,cAAc,CAACR,QAAD,CAApC;IACA,MAAMI,OAAO,CAACK,GAAR,CAAYF,OAAO,CAACjD,GAAR;MAAA,8BAAY,WAAOd,MAAP,EAAkB;QAC9C,MAAMC,UAAU,GAAGzB,IAAI,CAAC4C,IAAL,CAAUoC,QAAV,EAAoBxD,MAApB,CAAnB;QACA,MAAMkE,UAAU,SAASF,cAAc,CAAC/D,UAAD,CAAvC;QACA,MAAM2D,OAAO,CAACK,GAAR,CAAYC,UAAU,CAACpD,GAAX;UAAA,8BAAe,WAAOqD,SAAP,EAAqB;YACpD,MAAMC,aAAa,GAAG5F,IAAI,CAAC4C,IAAL,CAAUnB,UAAV,EAAsBkE,SAAtB,CAAtB,CADoD,CAGpD;;YACA,MAAME,gBAAgB,SAASL,cAAc,CAACI,aAAD,CAA7C;YACA,MAAMR,OAAO,CAACK,GAAR,CAAYI,gBAAgB,CAACvD,GAAjB;cAAA,8BAAqB,WAAOP,KAAP,EAAiB;gBACtD,MAAM+D,SAAS,GAAG9F,IAAI,CAAC4C,IAAL,CAAUgD,aAAV,EAAyB7D,KAAzB,CAAlB;;gBACA,IAAI;kBACF,MAAML,OAAO,SAASC,aAAa,CAACmE,SAAD,CAAnC,CADE,CAEF;kBACA;;kBACA,MAAMC,OAAO,GAAGrE,OAAO,CAAC4C,MAAR,CAAe,CAAC0B,GAAD,EAAMjE,KAAN,KAAgB;oBAC7CiE,GAAG,CAACC,GAAJ,CAAQlE,KAAK,CAACZ,GAAd,EAAmBY,KAAnB;oBACA,OAAOiE,GAAP;kBACD,CAHe,EAGb,IAAIE,GAAJ,EAHa,CAAhB,CAJE,CAQF;;kBACA,KAAK,MAAMnE,KAAX,IAAoBgE,OAAO,CAACI,MAAR,EAApB,EAAsC;oBACpC,MAAMC,SAAS,GAAGzC,WAAW,CAACzC,KAAD,EAAQa,KAAR,CAA7B;;oBACA,IAAIqE,SAAJ,EAAe;sBACblB,MAAM,CAAC7B,KAAP,CAAa+C,SAAb;oBACD;kBACF;gBACF,CAfD,CAeE,OAAO3C,GAAP,EAAY;kBACZ,IAAIA,GAAG,CAACrC,IAAJ,KAAa,QAAjB,EAA2B;oBACzB,OAAO+C,SAAP;kBACD;;kBACD,MAAMV,GAAN;gBACD;cACF,CAvBiB;;cAAA;gBAAA;cAAA;YAAA,IAAZ,CAAN;UAwBD,CA7BiB;;UAAA;YAAA;UAAA;QAAA,IAAZ,CAAN;MA8BD,CAjCiB;;MAAA;QAAA;MAAA;IAAA,IAAZ,CAAN;IAkCAyB,MAAM,CAACmB,GAAP;IACA,OAAOnB,MAAP;EACD,CAtCD,GAsCGoB,KAtCH,CAsCS7C,GAAG,IAAIyB,MAAM,CAACqB,IAAP,CAAY,OAAZ,EAAqB9C,GAArB,CAtChB;EAwCA,OAAOyB,MAAP;AACD;;AAEDrE,MAAM,CAACC,OAAP,CAAe0F,EAAf,GAAoBA,EAApB;;SAEeA,E;;;;;0BAAf,WAAmBtF,KAAnB,EAA0B;IACxB,MAAMQ,OAAO,SAASqD,QAAQ,CAAC7D,KAAD,CAAR,CAAgBuF,OAAhB,EAAtB;IACA,OAAO/E,OAAO,CAAC4C,MAAR,CAAe,CAAC0B,GAAD,EAAMU,EAAN,KAAa;MACjCV,GAAG,CAACU,EAAE,CAACvF,GAAJ,CAAH,GAAcuF,EAAd;MACA,OAAOV,GAAP;IACD,CAHM,EAGJ,EAHI,CAAP;EAID,C;;;;AAEDnF,MAAM,CAACC,OAAP,CAAea,aAAf,GAA+BA,aAA/B;;SAEeA,a;;;;;sCAAf,WAA8BH,MAA9B,EAAsCmF,MAAtC,EAA8C;IAC5C,MAAMC,IAAI,SAAS9G,EAAE,CAAC+G,QAAH,CAAYrF,MAAZ,EAAoB,MAApB,CAAnB;IACA,OAAOsF,cAAc,CAACF,IAAD,EAAOD,MAAP,CAArB;EACD,C;;;;AAED9F,MAAM,CAACC,OAAP,CAAea,aAAf,CAA6Bf,IAA7B,GAAoC8D,iBAApC;;AAEA,SAASA,iBAAT,CAA4BlD,MAA5B,EAAoCmF,MAApC,EAA4C;EAC1C,MAAMC,IAAI,GAAG9G,EAAE,CAACiH,YAAH,CAAgBvF,MAAhB,EAAwB,MAAxB,CAAb;EACA,OAAOsF,cAAc,CAACF,IAAD,EAAOD,MAAP,CAArB;AACD;;AAED,SAASG,cAAT,CAAyBF,IAAzB,EAA+BD,MAA/B,EAAuC;EACrC,MAAMjF,OAAO,GAAG,EAAhB;EACAkF,IAAI,CAACI,KAAL,CAAW,IAAX,EAAiBC,OAAjB,CAA0BlF,KAAD,IAAW;IAClC,IAAI,CAACA,KAAL,EAAY;MACV;IACD;;IAED,MAAMmF,MAAM,GAAGnF,KAAK,CAACiF,KAAN,CAAY,IAAZ,CAAf;;IACA,IAAI,CAACE,MAAM,CAAC,CAAD,CAAP,IAAcvE,SAAS,CAACuE,MAAM,CAAC,CAAD,CAAP,CAAT,KAAyBA,MAAM,CAAC,CAAD,CAAjD,EAAsD;MACpD;MACA;MACA;IACD;;IACD,IAAIC,GAAJ;;IACA,IAAI;MACFA,GAAG,GAAG3E,IAAI,CAAC4E,KAAL,CAAWF,MAAM,CAAC,CAAD,CAAjB,CAAN;IACD,CAFD,CAEE,OAAOG,CAAP,EAAU;MACV;MACA;IACD;;IACD,IAAIF,GAAJ,EAAS;MACPzF,OAAO,CAAC4F,IAAR,CAAaH,GAAb;IACD;EACF,CArBD;EAsBA,OAAOzF,OAAP;AACD;;AAEDb,MAAM,CAACC,OAAP,CAAemE,SAAf,GAA2BA,SAA3B;;AAEA,SAASA,SAAT,CAAoB/D,KAApB,EAA2B;EACzB,OAAOlB,IAAI,CAAC4C,IAAL,CAAU1B,KAAV,EAAkB,UAASZ,MAAO,EAAlC,CAAP;AACD;;AAEDO,MAAM,CAACC,OAAP,CAAeW,UAAf,GAA4BA,UAA5B;;AAEA,SAASA,UAAT,CAAqBP,KAArB,EAA4BC,GAA5B,EAAiC;EAC/B,MAAMoG,MAAM,GAAGC,OAAO,CAACrG,GAAD,CAAtB;EACA,OAAOnB,IAAI,CAAC4C,IAAL,CAAU6E,KAAV,CACLzH,IADK,EAEL,CAACiF,SAAS,CAAC/D,KAAD,CAAV,EAAmBwG,MAAnB,CAA0BrH,cAAc,CAACkH,MAAD,CAAxC,CAFK,CAAP;AAID;;AAED1G,MAAM,CAACC,OAAP,CAAe0G,OAAf,GAAyBA,OAAzB;;AAEA,SAASA,OAAT,CAAkBrG,GAAlB,EAAuB;EACrB,OAAOuB,IAAI,CAACvB,GAAD,EAAM,QAAN,CAAX;AACD;;AAEDN,MAAM,CAACC,OAAP,CAAe6B,SAAf,GAA2BA,SAA3B;;AAEA,SAASA,SAAT,CAAoBgF,GAApB,EAAyB;EACvB,OAAOjF,IAAI,CAACiF,GAAD,EAAM,MAAN,CAAX;AACD;;AAED,SAASjF,IAAT,CAAeiF,GAAf,EAAoBC,MAApB,EAA4B;EAC1B,OAAO/H,MAAM,CACVgI,UADI,CACOD,MADP,EAEJE,MAFI,CAEGH,GAFH,EAGJC,MAHI,CAGG,KAHH,CAAP;AAID;;AAED,SAASjE,WAAT,CAAsBzC,KAAtB,EAA6Ba,KAA7B,EAAoCgG,OAApC,EAA6C;EAC3C;EACA,IAAI,CAAChG,KAAK,CAACC,SAAP,IAAoB,CAAC+F,OAAzB,EAAkC;IAChC,OAAO,IAAP;EACD;;EAED,OAAO;IACL5G,GAAG,EAAEY,KAAK,CAACZ,GADN;IAELa,SAAS,EAAED,KAAK,CAACC,SAFZ;IAGLhC,IAAI,EAAE+B,KAAK,CAACC,SAAN,GAAkB7B,WAAW,CAACe,KAAD,EAAQa,KAAK,CAACC,SAAd,CAA7B,GAAwDmC,SAHzD;IAILL,IAAI,EAAE/B,KAAK,CAAC+B,IAJP;IAKLC,IAAI,EAAEhC,KAAK,CAACgC,IALP;IAMLF,QAAQ,EAAE9B,KAAK,CAAC8B;EANX,CAAP;AAQD;;AAED,SAAS2B,cAAT,CAAyBwC,GAAzB,EAA8B;EAC5B,OAAOlI,EAAE,CAACmI,OAAH,CAAWD,GAAX,EAAgB1B,KAAhB,CAAuB7C,GAAD,IAAS;IACpC,IAAIA,GAAG,CAACrC,IAAJ,KAAa,QAAb,IAAyBqC,GAAG,CAACrC,IAAJ,KAAa,SAA1C,EAAqD;MACnD,OAAO,EAAP;IACD;;IAED,MAAMqC,GAAN;EACD,CANM,CAAP;AAOD"},"metadata":{},"sourceType":"script"}